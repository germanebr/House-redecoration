# -*- coding: utf-8 -*-
"""General.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OSAJIK2-RG09WstH90W_1hgc6Cb-LGrN
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

from scipy.stats import multivariate_normal as mvn

def accuracy(y,y_hat):
  return np.mean(y==y_hat)

def R2(y, y_hat):
  return (1 - (np.sum((y - y_hat)**2) / np.sum((y - np.mean(y))**2)))

def coinFlip(y):
  y_hat = np.zeros(len(y))

  for i in range(len(y)):
    flip = np.random.randn(1)
    if flip > 0:
      y_hat[i] = 1
  
  return y_hat

def OLS(y, y_hat, N):
  return (1 / (2*N) * np.sum(y-y_hat)**2)

class contValData():
  #Set the random seed
  def random_seed(rseed):
    np.random.seed(rseed)

  def create(self, D, N, r=20):
    self.x = np.linspace(0, r, N).reshape(N, D)
    self.y = np.sqrt(self.x) + np.exp(-(self.x-5)**2) - 2*(np.exp(-(self.x-12.5)**2) + np.random.randn(N,1)*0.2)
    return self.x, self.y

  def show(self):
    plt.figure(figsize=(10,7))
    plt.scatter(self.x, self.y)

def confusionMatrix(actual, predicted):

    # extract the different classes
    classes = np.unique(actual)

    # initialize the confusion matrix
    confmat = np.zeros((len(classes), len(classes)))

    # loop across the different combinations of actual / predicted classes
    for i in range(len(classes)):
        for j in range(len(classes)):

           # count the number of instances in each combination of actual / predicted classes
           confmat[i, j] = np.sum((actual == classes[i]) & (predicted == classes[j]))

    return confmat

class KNNClassifier():
  def fit(self, x, y):
    self.x = x
    self.y = y

  def predict(self, x, k, epsilon=1e-3):
    N = len(x)  #Number of rows
    y_hat = np.zeros(N)

    for i in range(N):
      dist_sqr = np.sum((self.x - x[i])**2, axis=1) #Get the squared distance of each point
      idxt = np.argsort(dist_sqr)[:k] #Get the indexes of the K nearest neighbors
      gamma_k = 1 / (np.sqrt(dist_sqr[idxt]+epsilon)) #Get the weights

      y_hat[i] = np.bincount(self.y[idxt], weights=gamma_k).argmax()

    return y_hat

class GaussNB():
  def fit(self, x, y, epsilon=1e-3):
    self.likelihoods = dict()
    self.priors = dict()

    #Determine your classes
    self.K = set(y.astype(int))

    #Assign the x values to every given class
    for k in self.K:
      x_k = x[y==k,:]

      #Populate likelihoods
      self.likelihoods[k] = {"Mean" : x_k.mean(axis=0),
                             "Covariance" : x_k.var(axis=0) + epsilon}
      #populate priors (probability of x given y)
      self.priors[k] = len(x_k) / len(x)
    
  def predict(self, x):
    #Get number and dimension of observations
    N, D = x.shape

    #Get the predicted probability for every observation
    p_hat = np.zeros((N,len(self.K)))

    for k,l in self.likelihoods.items():
      p_hat[:,k] = mvn.logpdf(x, l['Mean'], l['Covariance']) + np.log(self.priors[k])

    return p_hat.argmax(axis=1)

class GaussBayes():
  def fit(self, x, y, epsilon=1e-3):
    self.likelihoods = dict()
    self.priors = dict()
    self.K = set(y.astype(int))

    #Set the covariance matrix
    for k in self.K:
      x_k = x[y==k,:]
      N_k, D = x_k.shape
      mu_k = x_k.mean(axis=0)
      
      self.likelihoods[k] = {'Mean' : mu_k,
                             'Covariance' : (1/(N_k-1)) * np.matmul((x_k - mu_k).T, x_k-mu_k) + epsilon*np.identity(D)}
      self.priors[k] = len(x_k) / len(x)

  def predict(self, x):
    #Get number and dimension of observations
    N, D = x.shape

    #Get the predicted probability for every observation
    p_hat = np.zeros((N,len(self.K)))

    for k,l in self.likelihoods.items():
      p_hat[:,k] = mvn.logpdf(x, l['Mean'], l['Covariance']) + np.log(self.priors[k])

    return p_hat.argmax(axis=1)

class KNNRegressor():
  def fit(self,x,y):
    self.x = x
    self.y = y

  def predict(self, x, K):
    N = len(x)
    y_hat = np.zeros(N)

    for i in range(N):
      dist2 = np.sum((self.x-x[i])**2, axis=1)
      idxt = np.argsort(dist2)[:K]
      gamma_K = np.exp(-dist2[idxt]) / np.exp(-dist2[idxt]).sum()
      y_hat[i] = gamma_K.dot(self.y[idxt])

    return y_hat

class SimpleLinearReg():
  def fit(self, x, y):
    self.y = y
    
    self.d = np.mean(x**2) - np.mean(x)**2  #Denominator for getting w0 and w1
    self.w0 = ((np.mean(y) * np.mean(x**2)) - (np.mean(x) * np.mean(x*y))) / self.d
    self.w1 = (np.mean(x*y) - (np.mean(x) * np.mean(y))) / self.d

  def predict(self, x, y, show=0):
    y_hat = self.w0 + (self.w1 * x)

    if show:
      plt.figure(figsize=(10,7))
      plt.scatter(x, y, s=8)
      plt.plot(x, y_hat, color='#FF0000')

    return y_hat

class MultipleLinearRegression():
  def fit(self, x, y):
    self.w = np.linalg.solve(x.T @ x, x.T @ y)

  def predict(self, x):
    return np.matmul(x, self.w)

class OurLinearRegression():
  def fit(self, x, y, eta=1e-3, epochs=1e3, show_curve=False):
    epochs = int(epochs)
    N, D = x.shape
    y = y

    #Stochastic initialization of weights
    self.W = np.random.randn(D)

    J = np.zeros(epochs)

    #Gradient descent for weight calculation
    for epoch in range(epochs):
      y_hat = self.predict(x)
      J[epoch] = OLS(y, y_hat, N) #Error calculation
      self.W -= eta*(1/N)*(x.T@(y_hat-y)) #Weight update rule

    if show_curve:
      plt.figure(figsize=(10,7))
      plt.plot(J)
      plt.xlabel('Epochs')
      plt.ylabel('$\mathcal{J}$')
      plt.title('Training Curve')
  
  def predict(self, x):
    return x@self.W